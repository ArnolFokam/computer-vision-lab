[
  {"id":"aroraTheoreticalAnalysisContrastive2019","abstract":"Recent empirical works have successfully used unlabeled data to learn feature representations that are broadly useful in downstream classification tasks. Several of these methods are reminiscent of the well-known word2vec embedding algorithm: leveraging availability of pairs of semantically \"similar\" data points and \"negative samples,\" the learner forces the inner product of representations of similar pairs with each other to be higher on average than with negative samples. The current paper uses the term contrastive learning for such algorithms and presents a theoretical framework for analyzing them by introducing latent classes and hypothesizing that semantically similar points are sampled from the same latent class. This framework allows us to show provable guarantees on the performance of the learned representations on the average classification task that is comprised of a subset of the same set of latent classes. Our generalization bound also shows that learned representations can reduce (labeled) sample complexity on downstream tasks. We conduct controlled experiments in both the text and image domains to support the theory.","accessed":{"date-parts":[[2023,2,1]]},"author":[{"family":"Arora","given":"Sanjeev"},{"family":"Khandeparkar","given":"Hrishikesh"},{"family":"Khodak","given":"Mikhail"},{"family":"Plevrakis","given":"Orestis"},{"family":"Saunshi","given":"Nikunj"}],"citation-key":"aroraTheoreticalAnalysisContrastive2019","issued":{"date-parts":[[2019,2,25]]},"number":"arXiv:1902.09229","publisher":"arXiv","source":"arXiv.org","title":"A Theoretical Analysis of Contrastive Unsupervised Representation Learning","type":"article","URL":"http://arxiv.org/abs/1902.09229"},
  {"id":"baiDirectionalSelfsupervisedLearning2021","abstract":"Despite the large augmentation family, only a few cherry-picked robust augmentation policies are beneficial to self-supervised image representation learning. In this paper, we propose a directional self-supervised learning paradigm (DSSL), which is compatible with significantly more augmentations. Specifically, we adapt heavy augmentation policies after the views lightly augmented by standard augmentations, to generate harder view (HV). HV usually has a higher deviation from the original image than the lightly augmented standard view (SV). Unlike previous methods equally pairing all augmented views to symmetrically maximize their similarities, DSSL treats augmented views of the same instance as a partially ordered set (with directions as SV$\\leftrightarrow $SV, SV$\\leftarrow$HV), and then equips a directional objective function respecting to the derived relationships among views. DSSL can be easily implemented with a few lines of codes and is highly flexible to popular self-supervised learning frameworks, including SimCLR, SimSiam, BYOL. Extensive experimental results on CIFAR and ImageNet demonstrated that DSSL can stably improve various baselines with compatibility to a wider range of augmentations.","accessed":{"date-parts":[[2023,2,17]]},"author":[{"family":"Bai","given":"Yalong"},{"family":"Yang","given":"Yifan"},{"family":"Zhang","given":"Wei"},{"family":"Mei","given":"Tao"}],"citation-key":"baiDirectionalSelfsupervisedLearning2021","issued":{"date-parts":[[2021,11,28]]},"number":"arXiv:2110.13555","publisher":"arXiv","source":"arXiv.org","title":"Directional Self-supervised Learning for Heavy Image Augmentations","type":"article","URL":"http://arxiv.org/abs/2110.13555"},
  {"id":"chenBigSelfSupervisedModels2020","abstract":"One paradigm for learning from few labeled examples while making best use of a large amount of unlabeled data is unsupervised pretraining followed by supervised fine-tuning. Although this paradigm uses unlabeled data in a task-agnostic way, in contrast to common approaches to semi-supervised learning for computer vision, we show that it is surprisingly effective for semi-supervised learning on ImageNet. A key ingredient of our approach is the use of big (deep and wide) networks during pretraining and fine-tuning. We find that, the fewer the labels, the more this approach (task-agnostic use of unlabeled data) benefits from a bigger network. After fine-tuning, the big network can be further improved and distilled into a much smaller one with little loss in classification accuracy by using the unlabeled examples for a second time, but in a task-specific way. The proposed semi-supervised learning algorithm can be summarized in three steps: unsupervised pretraining of a big ResNet model using SimCLRv2, supervised fine-tuning on a few labeled examples, and distillation with unlabeled examples for refining and transferring the task-specific knowledge. This procedure achieves 73.9% ImageNet top-1 accuracy with just 1% of the labels ($\\le$13 labeled images per class) using ResNet-50, a $10\\times$ improvement in label efficiency over the previous state-of-the-art. With 10% of labels, ResNet-50 trained with our method achieves 77.5% top-1 accuracy, outperforming standard supervised training with all of the labels.","accessed":{"date-parts":[[2022,12,30]]},"author":[{"family":"Chen","given":"Ting"},{"family":"Kornblith","given":"Simon"},{"family":"Swersky","given":"Kevin"},{"family":"Norouzi","given":"Mohammad"},{"family":"Hinton","given":"Geoffrey"}],"citation-key":"chenBigSelfSupervisedModels2020","issued":{"date-parts":[[2020,10,25]]},"number":"arXiv:2006.10029","publisher":"arXiv","source":"arXiv.org","title":"Big Self-Supervised Models are Strong Semi-Supervised Learners","type":"article","URL":"http://arxiv.org/abs/2006.10029"},
  {"id":"chenSimpleFrameworkContrastive2020","abstract":"This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.","accessed":{"date-parts":[[2022,12,30]]},"author":[{"family":"Chen","given":"Ting"},{"family":"Kornblith","given":"Simon"},{"family":"Norouzi","given":"Mohammad"},{"family":"Hinton","given":"Geoffrey"}],"citation-key":"chenSimpleFrameworkContrastive2020","issued":{"date-parts":[[2020,6,30]]},"number":"arXiv:2002.05709","publisher":"arXiv","source":"arXiv.org","title":"A Simple Framework for Contrastive Learning of Visual Representations","type":"article","URL":"http://arxiv.org/abs/2002.05709"},
  {"id":"chuangDebiasedContrastiveLearning2020","abstract":"A prominent technique for self-supervised representation learning has been to contrast semantically similar and dissimilar pairs of samples. Without access to labels, dissimilar (negative) points are typically taken to be randomly sampled datapoints, implicitly accepting that these points may, in reality, actually have the same label. Perhaps unsurprisingly, we observe that sampling negative examples from truly different labels improves performance, in a synthetic setting where labels are available. Motivated by this observation, we develop a debiased contrastive objective that corrects for the sampling of same-label datapoints, even without knowledge of the true labels. Empirically, the proposed objective consistently outperforms the state-of-the-art for representation learning in vision, language, and reinforcement learning benchmarks. Theoretically, we establish generalization bounds for the downstream classification task.","accessed":{"date-parts":[[2023,1,30]]},"author":[{"family":"Chuang","given":"Ching-Yao"},{"family":"Robinson","given":"Joshua"},{"family":"Yen-Chen","given":"Lin"},{"family":"Torralba","given":"Antonio"},{"family":"Jegelka","given":"Stefanie"}],"citation-key":"chuangDebiasedContrastiveLearning2020","issued":{"date-parts":[[2020,10,21]]},"number":"arXiv:2007.00224","publisher":"arXiv","source":"arXiv.org","title":"Debiased Contrastive Learning","type":"article","URL":"http://arxiv.org/abs/2007.00224"},
  {"id":"coleWhenDoesContrastive2022","abstract":"Recent self-supervised representation learning techniques have largely closed the gap between supervised and unsupervised learning on ImageNet classification. While the particulars of pretraining on ImageNet are now relatively well understood, the field still lacks widely accepted best practices for replicating this success on other datasets. As a first step in this direction, we study contrastive self-supervised learning on four diverse large-scale datasets. By looking through the lenses of data quantity, data domain, data quality, and task granularity, we provide new insights into the necessary conditions for successful self-supervised learning. Our key findings include observations such as: (i) the benefit of additional pretraining data beyond 500k images is modest, (ii) adding pretraining images from another domain does not lead to more general representations, (iii) corrupted pretraining images have a disparate impact on supervised and self-supervised pretraining, and (iv) contrastive learning lags far behind supervised learning on fine-grained visual classification tasks.","accessed":{"date-parts":[[2023,1,26]]},"author":[{"family":"Cole","given":"Elijah"},{"family":"Yang","given":"Xuan"},{"family":"Wilber","given":"Kimberly"},{"family":"Mac Aodha","given":"Oisin"},{"family":"Belongie","given":"Serge"}],"citation-key":"coleWhenDoesContrastive2022","issued":{"date-parts":[[2022,4,4]]},"number":"arXiv:2105.05837","publisher":"arXiv","source":"arXiv.org","title":"When Does Contrastive Visual Representation Learning Work?","type":"article","URL":"http://arxiv.org/abs/2105.05837"},
  {"id":"devriesDatasetAugmentationFeature2017","abstract":"Dataset augmentation, the practice of applying a wide array of domain-specific transformations to synthetically expand a training set, is a standard tool in supervised learning. While effective in tasks such as visual recognition, the set of transformations must be carefully designed, implemented, and tested for every new domain, limiting its re-use and generality. In this paper, we adopt a simpler, domain-agnostic approach to dataset augmentation. We start with existing data points and apply simple transformations such as adding noise, interpolating, or extrapolating between them. Our main insight is to perform the transformation not in input space, but in a learned feature space. A re-kindling of interest in unsupervised representation learning makes this technique timely and more effective. It is a simple proposal, but to-date one that has not been tested empirically. Working in the space of context vectors generated by sequence-to-sequence models, we demonstrate a technique that is effective for both static and sequential data.","accessed":{"date-parts":[[2022,12,9]]},"author":[{"family":"DeVries","given":"Terrance"},{"family":"Taylor","given":"Graham W."}],"citation-key":"devriesDatasetAugmentationFeature2017","issued":{"date-parts":[[2017,2,17]]},"number":"arXiv:1702.05538","publisher":"arXiv","source":"arXiv.org","title":"Dataset Augmentation in Feature Space","type":"article","URL":"http://arxiv.org/abs/1702.05538"},
  {"id":"duboisLossyCompressionLossless2022","abstract":"Most data is automatically collected and only ever \"seen\" by algorithms. Yet, data compressors preserve perceptual fidelity rather than just the information needed by algorithms performing downstream tasks. In this paper, we characterize the bit-rate required to ensure high performance on all predictive tasks that are invariant under a set of transformations, such as data augmentations. Based on our theory, we design unsupervised objectives for training neural compressors. Using these objectives, we train a generic image compressor that achieves substantial rate savings (more than $1000\\times$ on ImageNet) compared to JPEG on 8 datasets, without decreasing downstream classification performance.","accessed":{"date-parts":[[2022,12,9]]},"author":[{"family":"Dubois","given":"Yann"},{"family":"Bloem-Reddy","given":"Benjamin"},{"family":"Ullrich","given":"Karen"},{"family":"Maddison","given":"Chris J."}],"citation-key":"duboisLossyCompressionLossless2022","issued":{"date-parts":[[2022,1,28]]},"number":"arXiv:2106.10800","publisher":"arXiv","source":"arXiv.org","title":"Lossy Compression for Lossless Prediction","type":"article","URL":"http://arxiv.org/abs/2106.10800"},
  {"id":"finiSelfSupervisedModelsAre2022","abstract":"Self-supervised models have been shown to produce comparable or better visual representations than their supervised counterparts when trained offline on unlabeled data at scale. However, their efficacy is catastrophically reduced in a Continual Learning (CL) scenario where data is presented to the model sequentially. In this paper, we show that self-supervised loss functions can be seamlessly converted into distillation mechanisms for CL by adding a predictor network that maps the current state of the representations to their past state. This enables us to devise a framework for Continual self-supervised visual representation Learning that (i) significantly improves the quality of the learned representations, (ii) is compatible with several state-of-the-art self-supervised objectives, and (iii) needs little to no hyperparameter tuning. We demonstrate the effectiveness of our approach empirically by training six popular self-supervised models in various CL settings.","accessed":{"date-parts":[[2023,1,26]]},"author":[{"family":"Fini","given":"Enrico"},{"family":"Costa","given":"Victor G. Turrisi","non-dropping-particle":"da"},{"family":"Alameda-Pineda","given":"Xavier"},{"family":"Ricci","given":"Elisa"},{"family":"Alahari","given":"Karteek"},{"family":"Mairal","given":"Julien"}],"citation-key":"finiSelfSupervisedModelsAre2022","issued":{"date-parts":[[2022,4,1]]},"number":"arXiv:2112.04215","publisher":"arXiv","source":"arXiv.org","title":"Self-Supervised Models are Continual Learners","type":"article","URL":"http://arxiv.org/abs/2112.04215"},
  {"id":"grillBootstrapYourOwn2020","abstract":"We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches $74.3\\%$ top-1 classification accuracy on ImageNet using a linear evaluation with a ResNet-50 architecture and $79.6\\%$ with a larger ResNet. We show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks. Our implementation and pretrained models are given on GitHub.","accessed":{"date-parts":[[2023,2,23]]},"author":[{"family":"Grill","given":"Jean-Bastien"},{"family":"Strub","given":"Florian"},{"family":"Altché","given":"Florent"},{"family":"Tallec","given":"Corentin"},{"family":"Richemond","given":"Pierre H."},{"family":"Buchatskaya","given":"Elena"},{"family":"Doersch","given":"Carl"},{"family":"Pires","given":"Bernardo Avila"},{"family":"Guo","given":"Zhaohan Daniel"},{"family":"Azar","given":"Mohammad Gheshlaghi"},{"family":"Piot","given":"Bilal"},{"family":"Kavukcuoglu","given":"Koray"},{"family":"Munos","given":"Rémi"},{"family":"Valko","given":"Michal"}],"citation-key":"grillBootstrapYourOwn2020","issued":{"date-parts":[[2020,9,10]]},"number":"arXiv:2006.07733","publisher":"arXiv","source":"arXiv.org","title":"Bootstrap your own latent: A new approach to self-supervised Learning","title-short":"Bootstrap your own latent","type":"article","URL":"http://arxiv.org/abs/2006.07733"},
  {"id":"harwoodSmartMiningDeep2017","abstract":"To solve deep metric learning problems and producing feature embeddings, current methodologies will commonly use a triplet model to minimise the relative distance between samples from the same class and maximise the relative distance between samples from different classes. Though successful, the training convergence of this triplet model can be compromised by the fact that the vast majority of the training samples will produce gradients with magnitudes that are close to zero. This issue has motivated the development of methods that explore the global structure of the embedding and other methods that explore hard negative/positive mining. The effectiveness of such mining methods is often associated with intractable computational requirements. In this paper, we propose a novel deep metric learning method that combines the triplet model and the global structure of the embedding space. We rely on a smart mining procedure that produces effective training samples for a low computational cost. In addition, we propose an adaptive controller that automatically adjusts the smart mining hyper-parameters and speeds up the convergence of the training process. We show empirically that our proposed method allows for fast and more accurate training of triplet ConvNets than other competing mining methods. Additionally, we show that our method achieves new state-of-the-art embedding results for CUB-200-2011 and Cars196 datasets.","accessed":{"date-parts":[[2023,3,6]]},"author":[{"family":"Harwood","given":"Ben"},{"family":"G","given":"Vijay Kumar B."},{"family":"Carneiro","given":"Gustavo"},{"family":"Reid","given":"Ian"},{"family":"Drummond","given":"Tom"}],"citation-key":"harwoodSmartMiningDeep2017","issued":{"date-parts":[[2017,7,27]]},"number":"arXiv:1704.01285","publisher":"arXiv","source":"arXiv.org","title":"Smart Mining for Deep Metric Learning","type":"article","URL":"http://arxiv.org/abs/1704.01285"},
  {"id":"kalantidisHardNegativeMixing2020","abstract":"Contrastive learning has become a key component of self-supervised learning approaches for computer vision. By learning to embed two augmented versions of the same image close to each other and to push the embeddings of different images apart, one can train highly transferable visual representations. As revealed by recent studies, heavy data augmentation and large sets of negatives are both crucial in learning such representations. At the same time, data mixing strategies either at the image or the feature level improve both supervised and semi-supervised learning by synthesizing novel examples, forcing networks to learn more robust features. In this paper, we argue that an important aspect of contrastive learning, i.e., the effect of hard negatives, has so far been neglected. To get more meaningful negative samples, current top contrastive self-supervised learning approaches either substantially increase the batch sizes, or keep very large memory banks; increasing the memory size, however, leads to diminishing returns in terms of performance. We therefore start by delving deeper into a top-performing framework and show evidence that harder negatives are needed to facilitate better and faster learning. Based on these observations, and motivated by the success of data mixing, we propose hard negative mixing strategies at the feature level, that can be computed on-the-fly with a minimal computational overhead. We exhaustively ablate our approach on linear classification, object detection and instance segmentation and show that employing our hard negative mixing procedure improves the quality of visual representations learned by a state-of-the-art self-supervised learning method.","accessed":{"date-parts":[[2023,1,30]]},"author":[{"family":"Kalantidis","given":"Yannis"},{"family":"Sariyildiz","given":"Mert Bulent"},{"family":"Pion","given":"Noe"},{"family":"Weinzaepfel","given":"Philippe"},{"family":"Larlus","given":"Diane"}],"citation-key":"kalantidisHardNegativeMixing2020","issued":{"date-parts":[[2020,12,4]]},"number":"arXiv:2010.01028","publisher":"arXiv","source":"arXiv.org","title":"Hard Negative Mixing for Contrastive Learning","type":"article","URL":"http://arxiv.org/abs/2010.01028"},
  {"id":"leeSelfsupervisedLabelAugmentation2020","abstract":"Self-supervised learning, which learns by constructing artificial labels given only the input signals, has recently gained considerable attention for learning representations with unlabeled datasets, i.e., learning without any human-annotated supervision. In this paper, we show that such a technique can be used to significantly improve the model accuracy even under fully-labeled datasets. Our scheme trains the model to learn both original and self-supervised tasks, but is different from conventional multi-task learning frameworks that optimize the summation of their corresponding losses. Our main idea is to learn a single unified task with respect to the joint distribution of the original and self-supervised labels, i.e., we augment original labels via self-supervision of input transformation. This simple, yet effective approach allows to train models easier by relaxing a certain invariant constraint during learning the original and self-supervised tasks simultaneously. It also enables an aggregated inference which combines the predictions from different augmentations to improve the prediction accuracy. Furthermore, we propose a novel knowledge transfer technique, which we refer to as self-distillation, that has the effect of the aggregated inference in a single (faster) inference. We demonstrate the large accuracy improvement and wide applicability of our framework on various fully-supervised settings, e.g., the few-shot and imbalanced classification scenarios.","accessed":{"date-parts":[[2023,2,24]]},"author":[{"family":"Lee","given":"Hankook"},{"family":"Hwang","given":"Sung Ju"},{"family":"Shin","given":"Jinwoo"}],"citation-key":"leeSelfsupervisedLabelAugmentation2020","issued":{"date-parts":[[2020,6,29]]},"number":"arXiv:1910.05872","publisher":"arXiv","source":"arXiv.org","title":"Self-supervised Label Augmentation via Input Transformations","type":"article","URL":"http://arxiv.org/abs/1910.05872"},
  {"id":"miyaiRethinkingRotationSelfSupervised2022","abstract":"Rotation is frequently listed as a candidate for data augmentation in contrastive learning but seldom provides satisfactory improvements. We argue that this is because the rotated image is always treated as either positive or negative. The semantics of an image can be rotation-invariant or rotation-variant, so whether the rotated image is treated as positive or negative should be determined based on the content of the image. Therefore, we propose a novel augmentation strategy, adaptive Positive or Negative Data Augmentation (PNDA), in which an original and its rotated image are a positive pair if they are semantically close and a negative pair if they are semantically different. To achieve PNDA, we first determine whether rotation is positive or negative on an image-by-image basis in an unsupervised way. Then, we apply PNDA to contrastive learning frameworks. Our experiments showed that PNDA improves the performance of contrastive learning. The code is available at \\url{ https://github.com/AtsuMiyai/rethinking_rotation}.","accessed":{"date-parts":[[2023,2,27]]},"author":[{"family":"Miyai","given":"Atsuyuki"},{"family":"Yu","given":"Qing"},{"family":"Ikami","given":"Daiki"},{"family":"Irie","given":"Go"},{"family":"Aizawa","given":"Kiyoharu"}],"citation-key":"miyaiRethinkingRotationSelfSupervised2022","issued":{"date-parts":[[2022,11,24]]},"number":"arXiv:2210.12681","publisher":"arXiv","source":"arXiv.org","title":"Rethinking Rotation in Self-Supervised Contrastive Learning: Adaptive Positive or Negative Data Augmentation","title-short":"Rethinking Rotation in Self-Supervised Contrastive Learning","type":"article","URL":"http://arxiv.org/abs/2210.12681"},
  {"id":"pengCraftingBetterContrastive2022","abstract":"Recent self-supervised contrastive learning methods greatly benefit from the Siamese structure that aims at minimizing distances between positive pairs. For high performance Siamese representation learning, one of the keys is to design good contrastive pairs. Most previous works simply apply random sampling to make different crops of the same image, which overlooks the semantic information that may degrade the quality of views. In this work, we propose ContrastiveCrop, which could effectively generate better crops for Siamese representation learning. Firstly, a semantic-aware object localization strategy is proposed within the training process in a fully unsupervised manner. This guides us to generate contrastive views which could avoid most false positives (i.e., object vs. background). Moreover, we empirically find that views with similar appearances are trivial for the Siamese model training. Thus, a center-suppressed sampling is further designed to enlarge the variance of crops. Remarkably, our method takes a careful consideration of positive pairs for contrastive learning with negligible extra training overhead. As a plug-and-play and framework-agnostic module, ContrastiveCrop consistently improves SimCLR, MoCo, BYOL, SimSiam by 0.4% ~ 2.0% classification accuracy on CIFAR-10, CIFAR-100, Tiny ImageNet and STL-10. Superior results are also achieved on downstream detection and segmentation tasks when pre-trained on ImageNet-1K.","accessed":{"date-parts":[[2023,1,26]]},"author":[{"family":"Peng","given":"Xiangyu"},{"family":"Wang","given":"Kai"},{"family":"Zhu","given":"Zheng"},{"family":"Wang","given":"Mang"},{"family":"You","given":"Yang"}],"citation-key":"pengCraftingBetterContrastive2022","issued":{"date-parts":[[2022,3,29]]},"number":"arXiv:2202.03278","publisher":"arXiv","source":"arXiv.org","title":"Crafting Better Contrastive Views for Siamese Representation Learning","type":"article","URL":"http://arxiv.org/abs/2202.03278"},
  {"id":"robinsonContrastiveLearningHard2021","abstract":"How can you sample good negative examples for contrastive learning? We argue that, as with metric learning, contrastive learning of representations benefits from hard negative samples (i.e., points that are difficult to distinguish from an anchor point). The key challenge toward using hard negatives is that contrastive methods must remain unsupervised, making it infeasible to adopt existing negative sampling strategies that use true similarity information. In response, we develop a new family of unsupervised sampling methods for selecting hard negative samples where the user can control the hardness. A limiting case of this sampling results in a representation that tightly clusters each class, and pushes different classes as far apart as possible. The proposed method improves downstream performance across multiple modalities, requires only few additional lines of code to implement, and introduces no computational overhead.","accessed":{"date-parts":[[2023,1,30]]},"author":[{"family":"Robinson","given":"Joshua"},{"family":"Chuang","given":"Ching-Yao"},{"family":"Sra","given":"Suvrit"},{"family":"Jegelka","given":"Stefanie"}],"citation-key":"robinsonContrastiveLearningHard2021","issued":{"date-parts":[[2021,1,24]]},"number":"arXiv:2010.04592","publisher":"arXiv","source":"arXiv.org","title":"Contrastive Learning with Hard Negative Samples","type":"article","URL":"http://arxiv.org/abs/2010.04592"},
  {"id":"tianWhatMakesGood2020","abstract":"Contrastive learning between multiple views of the data has recently achieved state of the art performance in the field of self-supervised representation learning. Despite its success, the influence of different view choices has been less studied. In this paper, we use theoretical and empirical analysis to better understand the importance of view selection, and argue that we should reduce the mutual information (MI) between views while keeping task-relevant information intact. To verify this hypothesis, we devise unsupervised and semi-supervised frameworks that learn effective views by aiming to reduce their MI. We also consider data augmentation as a way to reduce MI, and show that increasing data augmentation indeed leads to decreasing MI and improves downstream classification accuracy. As a by-product, we achieve a new state-of-the-art accuracy on unsupervised pre-training for ImageNet classification ($73\\%$ top-1 linear readout with a ResNet-50). In addition, transferring our models to PASCAL VOC object detection and COCO instance segmentation consistently outperforms supervised pre-training. Code:http://github.com/HobbitLong/PyContrast","accessed":{"date-parts":[[2023,2,17]]},"author":[{"family":"Tian","given":"Yonglong"},{"family":"Sun","given":"Chen"},{"family":"Poole","given":"Ben"},{"family":"Krishnan","given":"Dilip"},{"family":"Schmid","given":"Cordelia"},{"family":"Isola","given":"Phillip"}],"citation-key":"tianWhatMakesGood2020","issued":{"date-parts":[[2020,12,18]]},"number":"arXiv:2005.10243","publisher":"arXiv","source":"arXiv.org","title":"What Makes for Good Views for Contrastive Learning?","type":"article","URL":"http://arxiv.org/abs/2005.10243"},
  {"id":"wangRethinkingMinimalSufficient2022","abstract":"Contrastive learning between different views of the data achieves outstanding success in the field of self-supervised representation learning and the learned representations are useful in broad downstream tasks. Since all supervision information for one view comes from the other view, contrastive learning approximately obtains the minimal sufficient representation which contains the shared information and eliminates the non-shared information between views. Considering the diversity of the downstream tasks, it cannot be guaranteed that all task-relevant information is shared between views. Therefore, we assume the non-shared task-relevant information cannot be ignored and theoretically prove that the minimal sufficient representation in contrastive learning is not sufficient for the downstream tasks, which causes performance degradation. This reveals a new problem that the contrastive learning models have the risk of over-fitting to the shared information between views. To alleviate this problem, we propose to increase the mutual information between the representation and input as regularization to approximately introduce more task-relevant information, since we cannot utilize any downstream task information during training. Extensive experiments verify the rationality of our analysis and the effectiveness of our method. It significantly improves the performance of several classic contrastive learning models in downstream tasks. Our code is available at https://github.com/Haoqing-Wang/InfoCL.","accessed":{"date-parts":[[2023,1,26]]},"author":[{"family":"Wang","given":"Haoqing"},{"family":"Guo","given":"Xun"},{"family":"Deng","given":"Zhi-Hong"},{"family":"Lu","given":"Yan"}],"citation-key":"wangRethinkingMinimalSufficient2022","issued":{"date-parts":[[2022,4,2]]},"number":"arXiv:2203.07004","publisher":"arXiv","source":"arXiv.org","title":"Rethinking Minimal Sufficient Representation in Contrastive Learning","type":"article","URL":"http://arxiv.org/abs/2203.07004"},
  {"id":"wangUnderstandingBehaviourContrastive2021","abstract":"Unsupervised contrastive learning has achieved outstanding success, while the mechanism of contrastive loss has been less studied. In this paper, we concentrate on the understanding of the behaviours of unsupervised contrastive loss. We will show that the contrastive loss is a hardness-aware loss function, and the temperature {\\tau} controls the strength of penalties on hard negative samples. The previous study has shown that uniformity is a key property of contrastive learning. We build relations between the uniformity and the temperature {\\tau} . We will show that uniformity helps the contrastive learning to learn separable features, however excessive pursuit to the uniformity makes the contrastive loss not tolerant to semantically similar samples, which may break the underlying semantic structure and be harmful to the formation of features useful for downstream tasks. This is caused by the inherent defect of the instance discrimination objective. Specifically, instance discrimination objective tries to push all different instances apart, ignoring the underlying relations between samples. Pushing semantically consistent samples apart has no positive effect for acquiring a prior informative to general downstream tasks. A well-designed contrastive loss should have some extents of tolerance to the closeness of semantically similar samples. Therefore, we find that the contrastive loss meets a uniformity-tolerance dilemma, and a good choice of temperature can compromise these two properties properly to both learn separable features and tolerant to semantically similar samples, improving the feature qualities and the downstream performances.","accessed":{"date-parts":[[2023,3,26]]},"author":[{"family":"Wang","given":"Feng"},{"family":"Liu","given":"Huaping"}],"citation-key":"wangUnderstandingBehaviourContrastive2021","issued":{"date-parts":[[2021,3,20]]},"number":"arXiv:2012.09740","publisher":"arXiv","source":"arXiv.org","title":"Understanding the Behaviour of Contrastive Loss","type":"article","URL":"http://arxiv.org/abs/2012.09740"},
  {"id":"wangUnderstandingContrastiveRepresentation2022","abstract":"Contrastive representation learning has been outstandingly successful in practice. In this work, we identify two key properties related to the contrastive loss: (1) alignment (closeness) of features from positive pairs, and (2) uniformity of the induced distribution of the (normalized) features on the hypersphere. We prove that, asymptotically, the contrastive loss optimizes these properties, and analyze their positive effects on downstream tasks. Empirically, we introduce an optimizable metric to quantify each property. Extensive experiments on standard vision and language datasets confirm the strong agreement between both metrics and downstream task performance. Remarkably, directly optimizing for these two metrics leads to representations with comparable or better performance at downstream tasks than contrastive learning. Project Page: https://tongzhouwang.info/hypersphere Code: https://github.com/SsnL/align_uniform , https://github.com/SsnL/moco_align_uniform","accessed":{"date-parts":[[2023,1,29]]},"author":[{"family":"Wang","given":"Tongzhou"},{"family":"Isola","given":"Phillip"}],"citation-key":"wangUnderstandingContrastiveRepresentation2022","issued":{"date-parts":[[2022,8,15]]},"number":"arXiv:2005.10242","publisher":"arXiv","source":"arXiv.org","title":"Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere","type":"article","URL":"http://arxiv.org/abs/2005.10242"},
  {"id":"wangUnsupervisedRepresentationLearning2020","author":[{"family":"Wang","given":"Feng"},{"family":"Liu","given":"Huaping"},{"family":"Guo","given":"Di"},{"family":"Fuchun","given":"Sun"}],"citation-key":"wangUnsupervisedRepresentationLearning2020","container-title":"Advances in neural information processing systems","editor":[{"family":"Larochelle","given":"H."},{"family":"Ranzato","given":"M."},{"family":"Hadsell","given":"R."},{"family":"Balcan","given":"M.F."},{"family":"Lin","given":"H."}],"issued":{"date-parts":[[2020]]},"page":"3510–3520","publisher":"Curran Associates, Inc.","title":"Unsupervised representation learning by invariance propagation","type":"paper-conference","URL":"https://proceedings.neurips.cc/paper/2020/file/23af4b45f1e166141a790d1a3126e77a-Paper.pdf","volume":"33"},
  {"id":"wilesCompressedVisionEfficient2022","abstract":"Experience and reasoning occur across multiple temporal scales: milliseconds, seconds, hours or days. The vast majority of computer vision research, however, still focuses on individual images or short videos lasting only a few seconds. This is because handling longer videos require more scalable approaches even to process them. In this work, we propose a framework enabling research on hour-long videos with the same hardware that can now process second-long videos. We replace standard video compression, e.g. JPEG, with neural compression and show that we can directly feed compressed videos as inputs to regular video networks. Operating on compressed videos improves efficiency at all pipeline levels -- data transfer, speed and memory -- making it possible to train models faster and on much longer videos. Processing compressed signals has, however, the downside of precluding standard augmentation techniques if done naively. We address that by introducing a small network that can apply transformations to latent codes corresponding to commonly used augmentations in the original video space. We demonstrate that with our compressed vision pipeline, we can train video models more efficiently on popular benchmarks such as Kinetics600 and COIN. We also perform proof-of-concept experiments with new tasks defined over hour-long videos at standard frame rates. Processing such long videos is impossible without using compressed representation.","accessed":{"date-parts":[[2022,12,9]]},"author":[{"family":"Wiles","given":"Olivia"},{"family":"Carreira","given":"Joao"},{"family":"Barr","given":"Iain"},{"family":"Zisserman","given":"Andrew"},{"family":"Malinowski","given":"Mateusz"}],"citation-key":"wilesCompressedVisionEfficient2022","issued":{"date-parts":[[2022,10,6]]},"number":"arXiv:2210.02995","publisher":"arXiv","source":"arXiv.org","title":"Compressed Vision for Efficient Video Understanding","type":"article","URL":"http://arxiv.org/abs/2210.02995"},
  {"id":"wuConditionalNegativeSampling2020","abstract":"Recent methods for learning unsupervised visual representations, dubbed contrastive learning, optimize the noise-contrastive estimation (NCE) bound on mutual information between two views of an image. NCE uses randomly sampled negative examples to normalize the objective. In this paper, we show that choosing difficult negatives, or those more similar to the current instance, can yield stronger representations. To do this, we introduce a family of mutual information estimators that sample negatives conditionally -- in a \"ring\" around each positive. We prove that these estimators lower-bound mutual information, with higher bias but lower variance than NCE. Experimentally, we find our approach, applied on top of existing models (IR, CMC, and MoCo) improves accuracy by 2-5% points in each case, measured by linear evaluation on four standard image datasets. Moreover, we find continued benefits when transferring features to a variety of new image distributions from the Meta-Dataset collection and to a variety of downstream tasks such as object detection, instance segmentation, and keypoint detection.","accessed":{"date-parts":[[2023,2,15]]},"author":[{"family":"Wu","given":"Mike"},{"family":"Mosse","given":"Milan"},{"family":"Zhuang","given":"Chengxu"},{"family":"Yamins","given":"Daniel"},{"family":"Goodman","given":"Noah"}],"citation-key":"wuConditionalNegativeSampling2020","issued":{"date-parts":[[2020,10,5]]},"number":"arXiv:2010.02037","publisher":"arXiv","source":"arXiv.org","title":"Conditional Negative Sampling for Contrastive Learning of Visual Representations","type":"article","URL":"http://arxiv.org/abs/2010.02037"},
  {"id":"wuUnsupervisedFeatureLearning2018","abstract":"Neural net classifiers trained on data with annotated class labels can also capture apparent visual similarity among categories without being directed to do so. We study whether this observation can be extended beyond the conventional domain of supervised learning: Can we learn a good feature representation that captures apparent similarity among instances, instead of classes, by merely asking the feature to be discriminative of individual instances? We formulate this intuition as a non-parametric classification problem at the instance-level, and use noise-contrastive estimation to tackle the computational challenges imposed by the large number of instance classes. Our experimental results demonstrate that, under unsupervised learning settings, our method surpasses the state-of-the-art on ImageNet classification by a large margin. Our method is also remarkable for consistently improving test performance with more training data and better network architectures. By fine-tuning the learned feature, we further obtain competitive results for semi-supervised learning and object detection tasks. Our non-parametric model is highly compact: With 128 features per image, our method requires only 600MB storage for a million images, enabling fast nearest neighbour retrieval at the run time.","accessed":{"date-parts":[[2023,1,30]]},"author":[{"family":"Wu","given":"Zhirong"},{"family":"Xiong","given":"Yuanjun"},{"family":"Yu","given":"Stella"},{"family":"Lin","given":"Dahua"}],"citation-key":"wuUnsupervisedFeatureLearning2018","issued":{"date-parts":[[2018,5,4]]},"number":"arXiv:1805.01978","publisher":"arXiv","source":"arXiv.org","title":"Unsupervised Feature Learning via Non-Parametric Instance-level Discrimination","type":"article","URL":"http://arxiv.org/abs/1805.01978"},
  {"id":"zhangRethinkingAugmentationModule2022","abstract":"A data augmentation module is utilized in contrastive learning to transform the given data example into two views, which is considered essential and irreplaceable. However, the predetermined composition of multiple data augmentations brings two drawbacks. First, the artificial choice of augmentation types brings specific representational invariances to the model, which have different degrees of positive and negative effects on different downstream tasks. Treating each type of augmentation equally during training makes the model learn non-optimal representations for various downstream tasks and limits the flexibility to choose augmentation types beforehand. Second, the strong data augmentations used in classic contrastive learning methods may bring too much invariance in some cases, and fine-grained information that is essential to some downstream tasks may be lost. This paper proposes a general method to alleviate these two problems by considering where and what to contrast in a general contrastive learning framework. We first propose to learn different augmentation invariances at different depths of the model according to the importance of each data augmentation instead of learning representational invariances evenly in the backbone. We then propose to expand the contrast content with augmentation embeddings to reduce the misleading effects of strong data augmentations. Experiments based on several baseline methods demonstrate that we learn better representations for various benchmarks on classification, detection, and segmentation downstream tasks.","accessed":{"date-parts":[[2023,1,26]]},"author":[{"family":"Zhang","given":"Junbo"},{"family":"Ma","given":"Kaisheng"}],"citation-key":"zhangRethinkingAugmentationModule2022","issued":{"date-parts":[[2022,8,21]]},"number":"arXiv:2206.00227","publisher":"arXiv","source":"arXiv.org","title":"Rethinking the Augmentation Module in Contrastive Learning: Learning Hierarchical Augmentation Invariance with Expanded Views","title-short":"Rethinking the Augmentation Module in Contrastive Learning","type":"article","URL":"http://arxiv.org/abs/2206.00227"}
]
