[
  {"id":"devriesDatasetAugmentationFeature2017","abstract":"Dataset augmentation, the practice of applying a wide array of domain-specific transformations to synthetically expand a training set, is a standard tool in supervised learning. While effective in tasks such as visual recognition, the set of transformations must be carefully designed, implemented, and tested for every new domain, limiting its re-use and generality. In this paper, we adopt a simpler, domain-agnostic approach to dataset augmentation. We start with existing data points and apply simple transformations such as adding noise, interpolating, or extrapolating between them. Our main insight is to perform the transformation not in input space, but in a learned feature space. A re-kindling of interest in unsupervised representation learning makes this technique timely and more effective. It is a simple proposal, but to-date one that has not been tested empirically. Working in the space of context vectors generated by sequence-to-sequence models, we demonstrate a technique that is effective for both static and sequential data.","accessed":{"date-parts":[[2022,12,9]]},"author":[{"family":"DeVries","given":"Terrance"},{"family":"Taylor","given":"Graham W."}],"citation-key":"devriesDatasetAugmentationFeature2017","issued":{"date-parts":[[2017,2,17]]},"number":"arXiv:1702.05538","publisher":"arXiv","source":"arXiv.org","title":"Dataset Augmentation in Feature Space","type":"article","URL":"http://arxiv.org/abs/1702.05538"},
  {"id":"duboisLossyCompressionLossless2022","abstract":"Most data is automatically collected and only ever \"seen\" by algorithms. Yet, data compressors preserve perceptual fidelity rather than just the information needed by algorithms performing downstream tasks. In this paper, we characterize the bit-rate required to ensure high performance on all predictive tasks that are invariant under a set of transformations, such as data augmentations. Based on our theory, we design unsupervised objectives for training neural compressors. Using these objectives, we train a generic image compressor that achieves substantial rate savings (more than $1000\\times$ on ImageNet) compared to JPEG on 8 datasets, without decreasing downstream classification performance.","accessed":{"date-parts":[[2022,12,9]]},"author":[{"family":"Dubois","given":"Yann"},{"family":"Bloem-Reddy","given":"Benjamin"},{"family":"Ullrich","given":"Karen"},{"family":"Maddison","given":"Chris J."}],"citation-key":"duboisLossyCompressionLossless2022","issued":{"date-parts":[[2022,1,28]]},"number":"arXiv:2106.10800","publisher":"arXiv","source":"arXiv.org","title":"Lossy Compression for Lossless Prediction","type":"article","URL":"http://arxiv.org/abs/2106.10800"},
  {"id":"wilesCompressedVisionEfficient2022","abstract":"Experience and reasoning occur across multiple temporal scales: milliseconds, seconds, hours or days. The vast majority of computer vision research, however, still focuses on individual images or short videos lasting only a few seconds. This is because handling longer videos require more scalable approaches even to process them. In this work, we propose a framework enabling research on hour-long videos with the same hardware that can now process second-long videos. We replace standard video compression, e.g. JPEG, with neural compression and show that we can directly feed compressed videos as inputs to regular video networks. Operating on compressed videos improves efficiency at all pipeline levels -- data transfer, speed and memory -- making it possible to train models faster and on much longer videos. Processing compressed signals has, however, the downside of precluding standard augmentation techniques if done naively. We address that by introducing a small network that can apply transformations to latent codes corresponding to commonly used augmentations in the original video space. We demonstrate that with our compressed vision pipeline, we can train video models more efficiently on popular benchmarks such as Kinetics600 and COIN. We also perform proof-of-concept experiments with new tasks defined over hour-long videos at standard frame rates. Processing such long videos is impossible without using compressed representation.","accessed":{"date-parts":[[2022,12,9]]},"author":[{"family":"Wiles","given":"Olivia"},{"family":"Carreira","given":"Joao"},{"family":"Barr","given":"Iain"},{"family":"Zisserman","given":"Andrew"},{"family":"Malinowski","given":"Mateusz"}],"citation-key":"wilesCompressedVisionEfficient2022","issued":{"date-parts":[[2022,10,6]]},"number":"arXiv:2210.02995","publisher":"arXiv","source":"arXiv.org","title":"Compressed Vision for Efficient Video Understanding","type":"article","URL":"http://arxiv.org/abs/2210.02995"}
]
