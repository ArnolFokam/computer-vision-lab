[
  {"id":"chenBigSelfSupervisedModels2020","abstract":"One paradigm for learning from few labeled examples while making best use of a large amount of unlabeled data is unsupervised pretraining followed by supervised fine-tuning. Although this paradigm uses unlabeled data in a task-agnostic way, in contrast to common approaches to semi-supervised learning for computer vision, we show that it is surprisingly effective for semi-supervised learning on ImageNet. A key ingredient of our approach is the use of big (deep and wide) networks during pretraining and fine-tuning. We find that, the fewer the labels, the more this approach (task-agnostic use of unlabeled data) benefits from a bigger network. After fine-tuning, the big network can be further improved and distilled into a much smaller one with little loss in classification accuracy by using the unlabeled examples for a second time, but in a task-specific way. The proposed semi-supervised learning algorithm can be summarized in three steps: unsupervised pretraining of a big ResNet model using SimCLRv2, supervised fine-tuning on a few labeled examples, and distillation with unlabeled examples for refining and transferring the task-specific knowledge. This procedure achieves 73.9% ImageNet top-1 accuracy with just 1% of the labels ($\\le$13 labeled images per class) using ResNet-50, a $10\\times$ improvement in label efficiency over the previous state-of-the-art. With 10% of labels, ResNet-50 trained with our method achieves 77.5% top-1 accuracy, outperforming standard supervised training with all of the labels.","accessed":{"date-parts":[[2022,12,30]]},"author":[{"family":"Chen","given":"Ting"},{"family":"Kornblith","given":"Simon"},{"family":"Swersky","given":"Kevin"},{"family":"Norouzi","given":"Mohammad"},{"family":"Hinton","given":"Geoffrey"}],"citation-key":"chenBigSelfSupervisedModels2020","issued":{"date-parts":[[2020,10,25]]},"number":"arXiv:2006.10029","publisher":"arXiv","source":"arXiv.org","title":"Big Self-Supervised Models are Strong Semi-Supervised Learners","type":"article","URL":"http://arxiv.org/abs/2006.10029"},
  {"id":"chenSimpleFrameworkContrastive2020","abstract":"This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.","accessed":{"date-parts":[[2022,12,30]]},"author":[{"family":"Chen","given":"Ting"},{"family":"Kornblith","given":"Simon"},{"family":"Norouzi","given":"Mohammad"},{"family":"Hinton","given":"Geoffrey"}],"citation-key":"chenSimpleFrameworkContrastive2020","issued":{"date-parts":[[2020,6,30]]},"number":"arXiv:2002.05709","publisher":"arXiv","source":"arXiv.org","title":"A Simple Framework for Contrastive Learning of Visual Representations","type":"article","URL":"http://arxiv.org/abs/2002.05709"},
  {"id":"coleWhenDoesContrastive2022","abstract":"Recent self-supervised representation learning techniques have largely closed the gap between supervised and unsupervised learning on ImageNet classification. While the particulars of pretraining on ImageNet are now relatively well understood, the field still lacks widely accepted best practices for replicating this success on other datasets. As a first step in this direction, we study contrastive self-supervised learning on four diverse large-scale datasets. By looking through the lenses of data quantity, data domain, data quality, and task granularity, we provide new insights into the necessary conditions for successful self-supervised learning. Our key findings include observations such as: (i) the benefit of additional pretraining data beyond 500k images is modest, (ii) adding pretraining images from another domain does not lead to more general representations, (iii) corrupted pretraining images have a disparate impact on supervised and self-supervised pretraining, and (iv) contrastive learning lags far behind supervised learning on fine-grained visual classification tasks.","accessed":{"date-parts":[[2023,1,26]]},"author":[{"family":"Cole","given":"Elijah"},{"family":"Yang","given":"Xuan"},{"family":"Wilber","given":"Kimberly"},{"family":"Mac Aodha","given":"Oisin"},{"family":"Belongie","given":"Serge"}],"citation-key":"coleWhenDoesContrastive2022","issued":{"date-parts":[[2022,4,4]]},"number":"arXiv:2105.05837","publisher":"arXiv","source":"arXiv.org","title":"When Does Contrastive Visual Representation Learning Work?","type":"article","URL":"http://arxiv.org/abs/2105.05837"},
  {"id":"devriesDatasetAugmentationFeature2017","abstract":"Dataset augmentation, the practice of applying a wide array of domain-specific transformations to synthetically expand a training set, is a standard tool in supervised learning. While effective in tasks such as visual recognition, the set of transformations must be carefully designed, implemented, and tested for every new domain, limiting its re-use and generality. In this paper, we adopt a simpler, domain-agnostic approach to dataset augmentation. We start with existing data points and apply simple transformations such as adding noise, interpolating, or extrapolating between them. Our main insight is to perform the transformation not in input space, but in a learned feature space. A re-kindling of interest in unsupervised representation learning makes this technique timely and more effective. It is a simple proposal, but to-date one that has not been tested empirically. Working in the space of context vectors generated by sequence-to-sequence models, we demonstrate a technique that is effective for both static and sequential data.","accessed":{"date-parts":[[2022,12,9]]},"author":[{"family":"DeVries","given":"Terrance"},{"family":"Taylor","given":"Graham W."}],"citation-key":"devriesDatasetAugmentationFeature2017","issued":{"date-parts":[[2017,2,17]]},"number":"arXiv:1702.05538","publisher":"arXiv","source":"arXiv.org","title":"Dataset Augmentation in Feature Space","type":"article","URL":"http://arxiv.org/abs/1702.05538"},
  {"id":"duboisLossyCompressionLossless2022","abstract":"Most data is automatically collected and only ever \"seen\" by algorithms. Yet, data compressors preserve perceptual fidelity rather than just the information needed by algorithms performing downstream tasks. In this paper, we characterize the bit-rate required to ensure high performance on all predictive tasks that are invariant under a set of transformations, such as data augmentations. Based on our theory, we design unsupervised objectives for training neural compressors. Using these objectives, we train a generic image compressor that achieves substantial rate savings (more than $1000\\times$ on ImageNet) compared to JPEG on 8 datasets, without decreasing downstream classification performance.","accessed":{"date-parts":[[2022,12,9]]},"author":[{"family":"Dubois","given":"Yann"},{"family":"Bloem-Reddy","given":"Benjamin"},{"family":"Ullrich","given":"Karen"},{"family":"Maddison","given":"Chris J."}],"citation-key":"duboisLossyCompressionLossless2022","issued":{"date-parts":[[2022,1,28]]},"number":"arXiv:2106.10800","publisher":"arXiv","source":"arXiv.org","title":"Lossy Compression for Lossless Prediction","type":"article","URL":"http://arxiv.org/abs/2106.10800"},
  {"id":"finiSelfSupervisedModelsAre2022","abstract":"Self-supervised models have been shown to produce comparable or better visual representations than their supervised counterparts when trained offline on unlabeled data at scale. However, their efficacy is catastrophically reduced in a Continual Learning (CL) scenario where data is presented to the model sequentially. In this paper, we show that self-supervised loss functions can be seamlessly converted into distillation mechanisms for CL by adding a predictor network that maps the current state of the representations to their past state. This enables us to devise a framework for Continual self-supervised visual representation Learning that (i) significantly improves the quality of the learned representations, (ii) is compatible with several state-of-the-art self-supervised objectives, and (iii) needs little to no hyperparameter tuning. We demonstrate the effectiveness of our approach empirically by training six popular self-supervised models in various CL settings.","accessed":{"date-parts":[[2023,1,26]]},"author":[{"family":"Fini","given":"Enrico"},{"family":"Costa","given":"Victor G. Turrisi","non-dropping-particle":"da"},{"family":"Alameda-Pineda","given":"Xavier"},{"family":"Ricci","given":"Elisa"},{"family":"Alahari","given":"Karteek"},{"family":"Mairal","given":"Julien"}],"citation-key":"finiSelfSupervisedModelsAre2022","issued":{"date-parts":[[2022,4,1]]},"number":"arXiv:2112.04215","publisher":"arXiv","source":"arXiv.org","title":"Self-Supervised Models are Continual Learners","type":"article","URL":"http://arxiv.org/abs/2112.04215"},
  {"id":"pengCraftingBetterContrastive2022","abstract":"Recent self-supervised contrastive learning methods greatly benefit from the Siamese structure that aims at minimizing distances between positive pairs. For high performance Siamese representation learning, one of the keys is to design good contrastive pairs. Most previous works simply apply random sampling to make different crops of the same image, which overlooks the semantic information that may degrade the quality of views. In this work, we propose ContrastiveCrop, which could effectively generate better crops for Siamese representation learning. Firstly, a semantic-aware object localization strategy is proposed within the training process in a fully unsupervised manner. This guides us to generate contrastive views which could avoid most false positives (i.e., object vs. background). Moreover, we empirically find that views with similar appearances are trivial for the Siamese model training. Thus, a center-suppressed sampling is further designed to enlarge the variance of crops. Remarkably, our method takes a careful consideration of positive pairs for contrastive learning with negligible extra training overhead. As a plug-and-play and framework-agnostic module, ContrastiveCrop consistently improves SimCLR, MoCo, BYOL, SimSiam by 0.4% ~ 2.0% classification accuracy on CIFAR-10, CIFAR-100, Tiny ImageNet and STL-10. Superior results are also achieved on downstream detection and segmentation tasks when pre-trained on ImageNet-1K.","accessed":{"date-parts":[[2023,1,26]]},"author":[{"family":"Peng","given":"Xiangyu"},{"family":"Wang","given":"Kai"},{"family":"Zhu","given":"Zheng"},{"family":"Wang","given":"Mang"},{"family":"You","given":"Yang"}],"citation-key":"pengCraftingBetterContrastive2022","issued":{"date-parts":[[2022,3,29]]},"number":"arXiv:2202.03278","publisher":"arXiv","source":"arXiv.org","title":"Crafting Better Contrastive Views for Siamese Representation Learning","type":"article","URL":"http://arxiv.org/abs/2202.03278"},
  {"id":"wangRethinkingMinimalSufficient2022","abstract":"Contrastive learning between different views of the data achieves outstanding success in the field of self-supervised representation learning and the learned representations are useful in broad downstream tasks. Since all supervision information for one view comes from the other view, contrastive learning approximately obtains the minimal sufficient representation which contains the shared information and eliminates the non-shared information between views. Considering the diversity of the downstream tasks, it cannot be guaranteed that all task-relevant information is shared between views. Therefore, we assume the non-shared task-relevant information cannot be ignored and theoretically prove that the minimal sufficient representation in contrastive learning is not sufficient for the downstream tasks, which causes performance degradation. This reveals a new problem that the contrastive learning models have the risk of over-fitting to the shared information between views. To alleviate this problem, we propose to increase the mutual information between the representation and input as regularization to approximately introduce more task-relevant information, since we cannot utilize any downstream task information during training. Extensive experiments verify the rationality of our analysis and the effectiveness of our method. It significantly improves the performance of several classic contrastive learning models in downstream tasks. Our code is available at https://github.com/Haoqing-Wang/InfoCL.","accessed":{"date-parts":[[2023,1,26]]},"author":[{"family":"Wang","given":"Haoqing"},{"family":"Guo","given":"Xun"},{"family":"Deng","given":"Zhi-Hong"},{"family":"Lu","given":"Yan"}],"citation-key":"wangRethinkingMinimalSufficient2022","issued":{"date-parts":[[2022,4,2]]},"number":"arXiv:2203.07004","publisher":"arXiv","source":"arXiv.org","title":"Rethinking Minimal Sufficient Representation in Contrastive Learning","type":"article","URL":"http://arxiv.org/abs/2203.07004"},
  {"id":"wangUnderstandingContrastiveRepresentation2022","abstract":"Contrastive representation learning has been outstandingly successful in practice. In this work, we identify two key properties related to the contrastive loss: (1) alignment (closeness) of features from positive pairs, and (2) uniformity of the induced distribution of the (normalized) features on the hypersphere. We prove that, asymptotically, the contrastive loss optimizes these properties, and analyze their positive effects on downstream tasks. Empirically, we introduce an optimizable metric to quantify each property. Extensive experiments on standard vision and language datasets confirm the strong agreement between both metrics and downstream task performance. Remarkably, directly optimizing for these two metrics leads to representations with comparable or better performance at downstream tasks than contrastive learning. Project Page: https://tongzhouwang.info/hypersphere Code: https://github.com/SsnL/align_uniform , https://github.com/SsnL/moco_align_uniform","accessed":{"date-parts":[[2023,1,29]]},"author":[{"family":"Wang","given":"Tongzhou"},{"family":"Isola","given":"Phillip"}],"citation-key":"wangUnderstandingContrastiveRepresentation2022","issued":{"date-parts":[[2022,8,15]]},"number":"arXiv:2005.10242","publisher":"arXiv","source":"arXiv.org","title":"Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere","type":"article","URL":"http://arxiv.org/abs/2005.10242"},
  {"id":"wilesCompressedVisionEfficient2022","abstract":"Experience and reasoning occur across multiple temporal scales: milliseconds, seconds, hours or days. The vast majority of computer vision research, however, still focuses on individual images or short videos lasting only a few seconds. This is because handling longer videos require more scalable approaches even to process them. In this work, we propose a framework enabling research on hour-long videos with the same hardware that can now process second-long videos. We replace standard video compression, e.g. JPEG, with neural compression and show that we can directly feed compressed videos as inputs to regular video networks. Operating on compressed videos improves efficiency at all pipeline levels -- data transfer, speed and memory -- making it possible to train models faster and on much longer videos. Processing compressed signals has, however, the downside of precluding standard augmentation techniques if done naively. We address that by introducing a small network that can apply transformations to latent codes corresponding to commonly used augmentations in the original video space. We demonstrate that with our compressed vision pipeline, we can train video models more efficiently on popular benchmarks such as Kinetics600 and COIN. We also perform proof-of-concept experiments with new tasks defined over hour-long videos at standard frame rates. Processing such long videos is impossible without using compressed representation.","accessed":{"date-parts":[[2022,12,9]]},"author":[{"family":"Wiles","given":"Olivia"},{"family":"Carreira","given":"Joao"},{"family":"Barr","given":"Iain"},{"family":"Zisserman","given":"Andrew"},{"family":"Malinowski","given":"Mateusz"}],"citation-key":"wilesCompressedVisionEfficient2022","issued":{"date-parts":[[2022,10,6]]},"number":"arXiv:2210.02995","publisher":"arXiv","source":"arXiv.org","title":"Compressed Vision for Efficient Video Understanding","type":"article","URL":"http://arxiv.org/abs/2210.02995"},
  {"id":"zhangRethinkingAugmentationModule2022","abstract":"A data augmentation module is utilized in contrastive learning to transform the given data example into two views, which is considered essential and irreplaceable. However, the predetermined composition of multiple data augmentations brings two drawbacks. First, the artificial choice of augmentation types brings specific representational invariances to the model, which have different degrees of positive and negative effects on different downstream tasks. Treating each type of augmentation equally during training makes the model learn non-optimal representations for various downstream tasks and limits the flexibility to choose augmentation types beforehand. Second, the strong data augmentations used in classic contrastive learning methods may bring too much invariance in some cases, and fine-grained information that is essential to some downstream tasks may be lost. This paper proposes a general method to alleviate these two problems by considering where and what to contrast in a general contrastive learning framework. We first propose to learn different augmentation invariances at different depths of the model according to the importance of each data augmentation instead of learning representational invariances evenly in the backbone. We then propose to expand the contrast content with augmentation embeddings to reduce the misleading effects of strong data augmentations. Experiments based on several baseline methods demonstrate that we learn better representations for various benchmarks on classification, detection, and segmentation downstream tasks.","accessed":{"date-parts":[[2023,1,26]]},"author":[{"family":"Zhang","given":"Junbo"},{"family":"Ma","given":"Kaisheng"}],"citation-key":"zhangRethinkingAugmentationModule2022","issued":{"date-parts":[[2022,8,21]]},"number":"arXiv:2206.00227","publisher":"arXiv","source":"arXiv.org","title":"Rethinking the Augmentation Module in Contrastive Learning: Learning Hierarchical Augmentation Invariance with Expanded Views","title-short":"Rethinking the Augmentation Module in Contrastive Learning","type":"article","URL":"http://arxiv.org/abs/2206.00227"}
]
