[
  {"id":"chenBigSelfSupervisedModels2020","abstract":"One paradigm for learning from few labeled examples while making best use of a large amount of unlabeled data is unsupervised pretraining followed by supervised fine-tuning. Although this paradigm uses unlabeled data in a task-agnostic way, in contrast to common approaches to semi-supervised learning for computer vision, we show that it is surprisingly effective for semi-supervised learning on ImageNet. A key ingredient of our approach is the use of big (deep and wide) networks during pretraining and fine-tuning. We find that, the fewer the labels, the more this approach (task-agnostic use of unlabeled data) benefits from a bigger network. After fine-tuning, the big network can be further improved and distilled into a much smaller one with little loss in classification accuracy by using the unlabeled examples for a second time, but in a task-specific way. The proposed semi-supervised learning algorithm can be summarized in three steps: unsupervised pretraining of a big ResNet model using SimCLRv2, supervised fine-tuning on a few labeled examples, and distillation with unlabeled examples for refining and transferring the task-specific knowledge. This procedure achieves 73.9% ImageNet top-1 accuracy with just 1% of the labels ($\\le$13 labeled images per class) using ResNet-50, a $10\\times$ improvement in label efficiency over the previous state-of-the-art. With 10% of labels, ResNet-50 trained with our method achieves 77.5% top-1 accuracy, outperforming standard supervised training with all of the labels.","accessed":{"date-parts":[[2022,12,30]]},"author":[{"family":"Chen","given":"Ting"},{"family":"Kornblith","given":"Simon"},{"family":"Swersky","given":"Kevin"},{"family":"Norouzi","given":"Mohammad"},{"family":"Hinton","given":"Geoffrey"}],"citation-key":"chenBigSelfSupervisedModels2020","issued":{"date-parts":[[2020,10,25]]},"number":"arXiv:2006.10029","publisher":"arXiv","source":"arXiv.org","title":"Big Self-Supervised Models are Strong Semi-Supervised Learners","type":"article","URL":"http://arxiv.org/abs/2006.10029"},
  {"id":"chenSimpleFrameworkContrastive2020","abstract":"This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.","accessed":{"date-parts":[[2022,12,30]]},"author":[{"family":"Chen","given":"Ting"},{"family":"Kornblith","given":"Simon"},{"family":"Norouzi","given":"Mohammad"},{"family":"Hinton","given":"Geoffrey"}],"citation-key":"chenSimpleFrameworkContrastive2020","issued":{"date-parts":[[2020,6,30]]},"number":"arXiv:2002.05709","publisher":"arXiv","source":"arXiv.org","title":"A Simple Framework for Contrastive Learning of Visual Representations","type":"article","URL":"http://arxiv.org/abs/2002.05709"},
  {"id":"devriesDatasetAugmentationFeature2017","abstract":"Dataset augmentation, the practice of applying a wide array of domain-specific transformations to synthetically expand a training set, is a standard tool in supervised learning. While effective in tasks such as visual recognition, the set of transformations must be carefully designed, implemented, and tested for every new domain, limiting its re-use and generality. In this paper, we adopt a simpler, domain-agnostic approach to dataset augmentation. We start with existing data points and apply simple transformations such as adding noise, interpolating, or extrapolating between them. Our main insight is to perform the transformation not in input space, but in a learned feature space. A re-kindling of interest in unsupervised representation learning makes this technique timely and more effective. It is a simple proposal, but to-date one that has not been tested empirically. Working in the space of context vectors generated by sequence-to-sequence models, we demonstrate a technique that is effective for both static and sequential data.","accessed":{"date-parts":[[2022,12,9]]},"author":[{"family":"DeVries","given":"Terrance"},{"family":"Taylor","given":"Graham W."}],"citation-key":"devriesDatasetAugmentationFeature2017","issued":{"date-parts":[[2017,2,17]]},"number":"arXiv:1702.05538","publisher":"arXiv","source":"arXiv.org","title":"Dataset Augmentation in Feature Space","type":"article","URL":"http://arxiv.org/abs/1702.05538"},
  {"id":"duboisLossyCompressionLossless2022","abstract":"Most data is automatically collected and only ever \"seen\" by algorithms. Yet, data compressors preserve perceptual fidelity rather than just the information needed by algorithms performing downstream tasks. In this paper, we characterize the bit-rate required to ensure high performance on all predictive tasks that are invariant under a set of transformations, such as data augmentations. Based on our theory, we design unsupervised objectives for training neural compressors. Using these objectives, we train a generic image compressor that achieves substantial rate savings (more than $1000\\times$ on ImageNet) compared to JPEG on 8 datasets, without decreasing downstream classification performance.","accessed":{"date-parts":[[2022,12,9]]},"author":[{"family":"Dubois","given":"Yann"},{"family":"Bloem-Reddy","given":"Benjamin"},{"family":"Ullrich","given":"Karen"},{"family":"Maddison","given":"Chris J."}],"citation-key":"duboisLossyCompressionLossless2022","issued":{"date-parts":[[2022,1,28]]},"number":"arXiv:2106.10800","publisher":"arXiv","source":"arXiv.org","title":"Lossy Compression for Lossless Prediction","type":"article","URL":"http://arxiv.org/abs/2106.10800"},
  {"id":"wilesCompressedVisionEfficient2022","abstract":"Experience and reasoning occur across multiple temporal scales: milliseconds, seconds, hours or days. The vast majority of computer vision research, however, still focuses on individual images or short videos lasting only a few seconds. This is because handling longer videos require more scalable approaches even to process them. In this work, we propose a framework enabling research on hour-long videos with the same hardware that can now process second-long videos. We replace standard video compression, e.g. JPEG, with neural compression and show that we can directly feed compressed videos as inputs to regular video networks. Operating on compressed videos improves efficiency at all pipeline levels -- data transfer, speed and memory -- making it possible to train models faster and on much longer videos. Processing compressed signals has, however, the downside of precluding standard augmentation techniques if done naively. We address that by introducing a small network that can apply transformations to latent codes corresponding to commonly used augmentations in the original video space. We demonstrate that with our compressed vision pipeline, we can train video models more efficiently on popular benchmarks such as Kinetics600 and COIN. We also perform proof-of-concept experiments with new tasks defined over hour-long videos at standard frame rates. Processing such long videos is impossible without using compressed representation.","accessed":{"date-parts":[[2022,12,9]]},"author":[{"family":"Wiles","given":"Olivia"},{"family":"Carreira","given":"Joao"},{"family":"Barr","given":"Iain"},{"family":"Zisserman","given":"Andrew"},{"family":"Malinowski","given":"Mateusz"}],"citation-key":"wilesCompressedVisionEfficient2022","issued":{"date-parts":[[2022,10,6]]},"number":"arXiv:2210.02995","publisher":"arXiv","source":"arXiv.org","title":"Compressed Vision for Efficient Video Understanding","type":"article","URL":"http://arxiv.org/abs/2210.02995"}
]
